{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テキスト生成\n",
    "- promptに続く文章が生成される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yutou\\Desktop\\work\\18_LLM\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "猫が大好物で、よく食べているという。また、お酒が飲めるのも好きである。好きな色はピンク。嫌いな食べ物はトマト。苦手な物はカボチャ、ニンニク、タマネギ、\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# モデル名（GPT-2 日本語版）\n",
    "model_name = \"colorfulscoop/gpt2-small-ja\"\n",
    "#model_name = \"rinna/japanese-gpt2-medium\"\n",
    "\n",
    "# デバイス設定（GPUがあればCUDA、なければCPU）\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# モデルとトークナイザーのロード\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "# 文章生成関数\n",
    "def generate_sentence(prompt=\"犬が\", max_length=50):\n",
    "    \"\"\"プロンプトにフォーマットを適用して短い日本語文を生成\"\"\"\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=50,  # 50トークンまで生成\n",
    "            min_length=20,  # 最低20トークン以上生成\n",
    "            do_sample=True,  # ランダムサンプリングを使用\n",
    "            temperature=0.7,  # 多様性を持たせる\n",
    "            top_p=0.9,  # 高確率の単語を優先\n",
    "            top_k=40,  # 40個の候補から選択\n",
    "            repetition_penalty=1.2,  # 同じ単語の繰り返しを抑制\n",
    "            no_repeat_ngram_size=2,  # 2単語以上の繰り返しを防止\n",
    "            length_penalty=1.0,  # 文章の長さを適切に維持\n",
    "            num_beams=3,  # ビームサーチの分岐数\n",
    "            early_stopping=True,  # 途中で適切なタイミングで終了\n",
    "            num_return_sequences=1,  # 1つの文章を出力\n",
    "            pad_token_id=tokenizer.eos_token_id  # 文末トークンを設定\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# 実行（プロンプトを指定可能）\n",
    "prompt = \"猫が\"\n",
    "text = generate_sentence(prompt)\n",
    "#text = text.split(\"。\")[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文章から単語を取得\n",
    "\n",
    "- 単語の取得\n",
    "- 文章の○○置換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "天気 - 品詞: 名詞,一般,*,* 一般名詞\n",
      "公園 - 品詞: 名詞,一般,*,* 一般名詞\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "# Janomeの形態素解析器を初期化\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# テスト用の文章\n",
    "text = \"今日は天気が良いので、公園で散歩をしました。\"\n",
    "\n",
    "# 解析\n",
    "for token in tokenizer.tokenize(text):\n",
    "    n = \"\".join(token.part_of_speech.split(\",\")[:2][::-1])\n",
    "    if n == \"一般名詞\":\n",
    "        print(f\"{token.surface} - 品詞: {token.part_of_speech}\", n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'句点記号'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_Noun_something(text):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "paths = glob.glob(\"C://Program Files (x86)/MeCab/dic/ipadic/Noun.csv\")\n",
    "df = pd.concat([pd.read_csv(path, encoding=\"shift-jis\", header=None) for path in tqdm(paths)]).reset_index(drop=True).reset_index()\n",
    "df = df.iloc[:, [0, 1, 5, 6, 12]]\n",
    "df.columns = [\"id\", \"word\", \"type\", \"type_detail\", \"kana\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ドナーカード ドナーカードを提示すると、そのカードに書かれている内容が表示される\n"
     ]
    }
   ],
   "source": [
    "for word in random.sample(df[\"word\"].tolist(), 100):\n",
    "    #print(word)\n",
    "    text = generate_sentence(word).split(\"。\")[0]\n",
    "    print(word, text)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
