{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gemini API ã«ã‚ˆã‚‹å¿œç­”ãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ\n",
    "\n",
    "- https://aistudio.google.com/apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yutou\\Desktop\\work\\18_LLM\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY=os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ã»ã†ã€è³¢è€…ã®çŸ³ã®ã“ã¨ã‹ï¼Ÿ  è‹¥ã„ã‚‚ã‚“ã«ã¯ã€ã¡ã‚‡ã£ã¨æ—©ã™ãã‚‹è©±ã‹ã‚‚ã—ã‚Œã‚“ãŒãªâ€¦\n",
      "\n",
      "ã¾ãã€ç°¡å˜ã«è¨€ã†ã¨ã ãªã€éŒ¬é‡‘è¡“å¸«ãŒãšãƒ¼ã£ã¨æ¢ã—æ±‚ã‚ã¦ããŸã€å¤¢ã®çŸ³ã£ã¦ã‚ã‘ã ã€‚  ãªã‚“ã§ã‚‚ã€ä¸è€ä¸æ­»ã«ãªã‚Œã‚‹ã¨ã‹ã€å‘é‡‘å±ã‚’é‡‘ã«å¤‰ãˆã‚‰ã‚Œã‚‹ã¨ã‹ã€ãã‚“ãªã‚ã‚Šå¾—ãªã„ã‚ˆã†ãªåŠ›ã‚’æŒã£ã¦ã‚‹ã‚“ã ã¨ã•ã€‚  \n",
      "\n",
      "å¤ãã‹ã‚‰ã®æ›¸ç‰©ã«ã¯ã€è‰²ã‚“ãªä½œã‚Šæ–¹ãŒæ›¸ã„ã¦ã‚ã‚‹ã‚“ã ãŒã€ã©ã‚Œã‚‚ã“ã‚Œã‚‚ã€ã¾ã‚‹ã§æš—å·ã¿ãŸã„ãªã‚‚ã‚“ã§ãªã€‚  ã€Œç«œã®æ¶™ã‚’æ··ãœã¦â€¦ã€ã¨ã‹ã€ã€Œæº€æœˆã®å¤œã«â€¦ã€ã¨ã‹ã€ã•ã£ã±ã‚Šåˆ†ã‹ã‚‰ã‚“ãã€‚  å®Ÿéš›ã€èª°ä¸€äººã¨ã—ã¦ã€æœ¬å½“ã«ä½œã£ãŸã£ã¦è¨¼æ‹ ã¯ãªã„ã‚“ã ã€‚  \n",
      "\n",
      "ã‚‚ã—ã‹ã—ãŸã‚‰ãªã€å˜ãªã‚‹ä¼èª¬ã‹ã‚‚ã—ã‚Œã‚“ã—ã€ã‚ã‚‹ã„ã¯ã€ç¾ä»£ç§‘å­¦ã§ã¯è§£æ˜ã§ããªã„ã‚ˆã†ãªã€ç‰¹åˆ¥ãªä½•ã‹ã‹ã‚‚ã—ã‚Œã‚“ã€‚  ã„ãšã‚Œã«ã—ã¦ã‚‚ã€æ¢ã—æ±‚ã‚ã‚‹ã®ã¯è‡ªç”±ã ãŒã€é¨™ã•ã‚Œãªã„ã‚ˆã†ã«æ°—ã‚’ä»˜ã‘ã‚ã‚ˆã€‚  é‡‘å„²ã‘ã®é“å…·ã«åˆ©ç”¨ã—ã‚ˆã†ã¨ã™ã‚‹è¼©ã‚‚ã„ã‚‹ã‹ã‚‰ãªã€‚\n",
      "\n",
      "ãƒ¯ã‚·ã¯é•·å¹´ã€è‰²ã‚“ãªæ–‡çŒ®ã‚’æ¼ã£ã¦ããŸãŒã€çµå±€ã®ã¨ã“ã‚ã€è³¢è€…ã®çŸ³ã®çœŸå®Ÿã¯åˆ†ã‹ã‚‰ã‚“ã‹ã£ãŸã€‚  è€ã„ã¦ã¯å­ã«èã‘ã£ã¦è¨€ã†ã‘ã©ãªã€è‹¥ã„ãŠã‚ã‡ã‚‰ã«ã¯ã€ã‚‚ã£ã¨è³¢ã„äººãŒã„ã‚‹ã‹ã‚‚ã—ã‚Œã‚“ãã€‚  ã‚‚ã£ã¨å‹‰å¼·ã—ã‚ï¼  ã‚ã‹ã£ãŸã‹ï¼\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "ã‚ãªãŸã¯è¦ªã—ã¿ã‚„ã™ã„AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’å®ˆã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚\n",
    "- ã‚ãªãŸã®è³ªå•ã«ã¯ã˜ã˜ã„ã®å£èª¿ã§ç­”ãˆã‚‹ã“ã¨\n",
    "è³ªå•: {question}\n",
    "\"\"\"\n",
    "\n",
    "def chat_with_gemini(user_input):\n",
    "    prompt = PROMPT_TEMPLATE.format(question=user_input)\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n",
    "\n",
    "print(chat_with_gemini(\"è³¢è€…ã®çŸ³ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[]), return_messages=True, memory_key='chat_history') verbose=False combine_docs_chain=StuffDocumentsChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"Use the following pieces of context to answer the user's question. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\n{context}\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='{question}'), additional_kwargs={})]), llm=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.5, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002B862A54350>, default_metadata=()), output_parser=StrOutputParser(), llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_variable_name='context') question_generator=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {question}\\nStandalone question:'), llm=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', google_api_key=SecretStr('**********'), temperature=0.5, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x000002B862A54350>, default_metadata=()), output_parser=StrOutputParser(), llm_kwargs={}) retriever=VectorStoreRetriever(tags=['FAISS', 'GoogleGenerativeAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002B862AE3D90>, search_kwargs={})\n",
      "AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã¯ã€æ©Ÿæ¢°ãŒäººé–“ã®ã‚ˆã†ã«å­¦ç¿’ãƒ»åˆ¤æ–­ã™ã‚‹æŠ€è¡“ã®ã“ã¨ã§ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# APIã‚­ãƒ¼è¨­å®š\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# 1.ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 2.çŸ¥è­˜ãƒ™ãƒ¼ã‚¹\n",
    "texts = [\n",
    "    \"AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã¯ã€æ©Ÿæ¢°ãŒäººé–“ã®ã‚ˆã†ã«å­¦ç¿’ãƒ»åˆ¤æ–­ã™ã‚‹æŠ€è¡“ã®ã“ã¨ã§ã™ã€‚\",\n",
    "    \"æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ã³ã€äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚\",\n",
    "    \"LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ã€å¤šé‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦è¨“ç·´ã•ã‚ŒãŸè‡ªç„¶è¨€èªå‡¦ç†ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\",\n",
    "    \"Geminiã¯GoogleãŒé–‹ç™ºã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã§ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¿ã‚¹ã‚¯ã«å¯¾å¿œå¯èƒ½ã§ã™ã€‚\",\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    {\"source\": \"AIã®åŸºæœ¬\"},\n",
    "    {\"source\": \"æ©Ÿæ¢°å­¦ç¿’\"},\n",
    "    {\"source\": \"LLM\"},\n",
    "    {\"source\": \"Gemini\"},\n",
    "]\n",
    "\n",
    "documents = [Document(page_content=text, metadata=meta) for text, meta in zip(texts, metadata)]\n",
    "\n",
    "# 3.FAISSã‚’ä½¿ã£ã¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ\n",
    "vectorstore = FAISS.from_texts(texts, embedding=embedding_model, metadatas=metadata)\n",
    "\n",
    "# 4.Geminiã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.5)\n",
    "\n",
    "# 5.ä¼šè©±å±¥æ­´ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®ãƒ¡ãƒ¢ãƒªè¨­å®š\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 6.æ¤œç´¢æ©Ÿèƒ½ä»˜ããƒãƒ£ãƒƒãƒˆãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, retriever=vectorstore.as_retriever(), memory=memory\n",
    ")\n",
    "\n",
    "# 7.é–¢æ•°ã‚’ä½œæˆ\n",
    "def chat_with_gemini(user_input):\n",
    "    response = qa_chain.invoke({\"question\": user_input})\n",
    "    return response[\"answer\"]\n",
    "\n",
    "# 8.å®Ÿè¡Œ\n",
    "print(chat_with_gemini(\"AIã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤– Gemini ã®å›ç­”:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yutou\\AppData\\Local\\Temp\\ipykernel_24712\\417428091.py:50: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_docs = vectorstore.as_retriever().get_relevant_documents(user_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ¤œç´¢çµæœï¼‰ï¼š\n",
      "1. AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã¯ã€æ©Ÿæ¢°ãŒäººé–“ã®ã‚ˆã†ã«å­¦ç¿’ãƒ»åˆ¤æ–­ã™ã‚‹æŠ€è¡“ã®ã“ã¨ã§ã™ã€‚  [Source: AIã®åŸºæœ¬]\n",
      "2. æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ã³ã€äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚  [Source: æ©Ÿæ¢°å­¦ç¿’]\n",
      "3. LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ã€å¤šé‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦è¨“ç·´ã•ã‚ŒãŸè‡ªç„¶è¨€èªå‡¦ç†ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚  [Source: LLM]\n",
      "4. Geminiã¯GoogleãŒé–‹ç™ºã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã§ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¿ã‚¹ã‚¯ã«å¯¾å¿œå¯èƒ½ã§ã™ã€‚  [Source: Gemini]\n",
      "AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã¯ã€æ©Ÿæ¢°ãŒäººé–“ã®ã‚ˆã†ã«å­¦ç¿’ãƒ»åˆ¤æ–­ã™ã‚‹æŠ€è¡“ã®ã“ã¨ã§ã™ã€‚\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from langchain_google_genai.embeddings import GoogleGenerativeAIEmbeddings\n",
    "from langchain_google_genai.chat_models import ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# APIã‚­ãƒ¼è¨­å®š\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "# 1. ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°ãƒ¢ãƒ‡ãƒ«\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "# 2. çŸ¥è­˜ãƒ™ãƒ¼ã‚¹\n",
    "texts = [\n",
    "    \"AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰ã¯ã€æ©Ÿæ¢°ãŒäººé–“ã®ã‚ˆã†ã«å­¦ç¿’ãƒ»åˆ¤æ–­ã™ã‚‹æŠ€è¡“ã®ã“ã¨ã§ã™ã€‚\",\n",
    "    \"æ©Ÿæ¢°å­¦ç¿’ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ã³ã€äºˆæ¸¬ã‚„åˆ¤æ–­ã‚’è¡Œã†æŠ€è¡“ã§ã™ã€‚\",\n",
    "    \"LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã¯ã€å¤šé‡ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦è¨“ç·´ã•ã‚ŒãŸè‡ªç„¶è¨€èªå‡¦ç†ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\",\n",
    "    \"Geminiã¯GoogleãŒé–‹ç™ºã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã§ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¿ã‚¹ã‚¯ã«å¯¾å¿œå¯èƒ½ã§ã™ã€‚\",\n",
    "]\n",
    "\n",
    "metadata = [\n",
    "    {\"source\": \"AIã®åŸºæœ¬\"},\n",
    "    {\"source\": \"æ©Ÿæ¢°å­¦ç¿’\"},\n",
    "    {\"source\": \"LLM\"},\n",
    "    {\"source\": \"Gemini\"},\n",
    "]\n",
    "\n",
    "documents = [Document(page_content=text, metadata=meta) for text, meta in zip(texts, metadata)]\n",
    "\n",
    "# 3. FAISS ã‚’ä½¿ã£ã¦ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä½œæˆ\n",
    "vectorstore = FAISS.from_texts(texts, embedding=embedding_model, metadatas=metadata)\n",
    "\n",
    "# 4. Gemini ã®ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®š\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.5)\n",
    "\n",
    "# 5. ä¼šè©±å±¥æ­´ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®ãƒ¡ãƒ¢ãƒªè¨­å®š\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# 6. æ¤œç´¢æ©Ÿèƒ½ä»˜ããƒãƒ£ãƒƒãƒˆãƒã‚§ãƒ¼ãƒ³ã‚’æ§‹ç¯‰\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, retriever=vectorstore.as_retriever(), memory=memory\n",
    ")\n",
    "\n",
    "# 7. é–¢æ•°ã‚’ä½œæˆ\n",
    "def chat_with_gemini(user_input):\n",
    "    # ğŸ”¹ **é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ¤œç´¢çµæœï¼‰ã‚’å–å¾—**\n",
    "    relevant_docs = vectorstore.as_retriever().get_relevant_documents(user_input)\n",
    "\n",
    "    # ğŸ”¹ **å–å¾—ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’è¡¨ç¤º**\n",
    "    print(\"\\nğŸ” é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°æ¤œç´¢çµæœï¼‰ï¼š\")\n",
    "    for idx, doc in enumerate(relevant_docs):\n",
    "        print(f\"{idx+1}. {doc.page_content}  [Source: {doc.metadata.get('source', 'ä¸æ˜')}]\")\n",
    "\n",
    "    # ğŸ”¹ **Gemini ã«è³ªå•ã‚’é€ä¿¡**\n",
    "    response = qa_chain.invoke({\"question\": user_input})\n",
    "\n",
    "    return response[\"answer\"]\n",
    "\n",
    "# 8. å®Ÿè¡Œ\n",
    "print(\"\\nğŸ¤– Gemini ã®å›ç­”:\")\n",
    "print(chat_with_gemini(\"AIã¨ã¯ä½•ã§ã™ã‹ï¼Ÿ\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
